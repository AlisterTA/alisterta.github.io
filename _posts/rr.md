---
layout: post
title: سال 2016 در یک نگاه- شبکه های عصبی عمیق با عمق تصادفی-بخش اول
tags: [ResNet,DenseNet,FractalNet,ROR,کانولوشن]
---
شبکه های Residual امروزه مورد توجه بسیاری از محققین داده قرار گرفته است به طوری که بعد از معرفی آنها، معماری های مختلفی بر مبنای این شبکه ها ارائه شدند، 
به طور مثال میتوان به [Wide ResNet](https://arxiv.org/abs/1605.07146)،[ResNet in ResNet](https://arxiv.org/abs/1603.08029)،[Stochastic Depth ResNet](https://arxiv.org/abs/1603.09382) وغیره اشاره کرد وتصور برخی از محققین این است که برای داشتن یک شبکه خیلی عمیق نیاز است که حتما از ساختار Residual  استفاده کرد.

**مقاله دوم: [شبکه فرکتال]( https://arxiv.org/abs/1605.07648)(شبکه های خیلی عمیق بدون بلاک Residual)**

در این مقاله مولفین [گستاو لارسون](http://people.cs.uchicago.edu/~larsson/) و همکارانش یک معماری جدید به نام معماری شبکه فرکتال را ارایه کردند که نشان میدهد برای داشتن شبکه های خیلی عمیق الزاما نباید از معماری Residual استفاده کرد. این معماری نتایج شبکه های ResNet را در یک ساختار فرکتالی بدست می آورد. هر چند در حال حاضر از این متد خیلی کم استفاده میشود ، اما من فکر میکنم که ایده مطرح شده جالب است و ارزش مطالعه کردن را دارد.

**فرکتال**

فرکتال یک شکل هندسی چند جزئی است که می‌توان آن را به قسمت هایی تقسیم کرد، به طوری که هر قسمت یک کپی از کل  شکل باشد و دارای دو خصوصیت ویژه است . اول در ظاهر نامنظم و شکسته است ، دوم اینکه خو متشابه است. این عبارت برای اولین بار توسط ریاضی  دان لهستانی-فرانسوی [بنوا مندلبرو](https://en.wikipedia.org/wiki/Benoit_Mandelbrot) کشف شد. در صورتی که علاقه مند هستید بیشتر در مورد فرکتالها بدانید میتوانید سخن رانی تد ایشان با عنوان فرکتال ها و هنر ناهمواری ها را در [این لینک](https://www.youtube.com/watch?v=ay8OMOsf6AQ) دنبال کنید.

![_config.yml]({{ site.baseurl }}/images/13/1.png)

هر بلاک در این مدل ساختاری فرکتال مانند دارند که در هر مسیر تعداد لایه ها با ضریبی از دو افزاش پیدا میکند. این مدل از یک قانون ساده برای ایجاد یک معماری فرکتالی استفاده میکند. در حالت اولیه (سمت چپ) f1(z) شامل یک لایه (به عنوان مثال کانولوشن ) است. در شکل سمت راست که نسخه فرکتالی این مدل ساده است از fc به عنوان بلاک سازنده این مدل استفاده میکند با قرار دادن تعداد B  بلاک بر روی یکدیگر به معماری فرکتال گونه ای دست پیدا میکنیم که عمق آن توسط فرمول زیر محاسبه میشود. B x2^(C-1) که C تعداد ستون ها(یا مسیر های عبور داده) و B تعداد بلاک ها است . در شکل بالا عمق شبکه 40 است.

![_config.yml]({{ site.baseurl }}/images/13/2.png)

در این معماری از متدی به نام drop-path استفاده میشود که تاثیری مشابه مقاله شبکه های عصبی عمیق با عمق تصادفی  دارد. این متد مشابه Dropout است با این تفاوت که به جای حذف نود یک مسیر از شبکه حذف میشود . با این متد شبکه برای پیش پیش بینی خروجی فقط روی یک مسیر خاص تکیه نمیکند و مسیر های به تنهایی در پیش بینی خروجی نقش واحدی را دارند. این متد به دو شکل Local  و General انجام میشود که در شکل Global آن فقط یک مسیر برای پیش بینی خروجی انتخاب میشود که این مسیر یک ستون خاص از بلاک است بدین ترتیب هر ستون به تنهایی میتواند خروجی را پیش بینی کند . در شکل Local آن یک اتصال از ستون حذف میشود اما مطمین میشویم که حد اقل یکی از اتصال ها به صورت فعال باقی میماند.

![_config.yml]({{ site.baseurl }}/images/13/3.png)

نتجزییات بیشتر در مقاله شرح داده شده است .  این مقاله نتایج مشابه معماری ResNet را بدست آورد و از دو جهت مشابه معماری ذکر شده است : هر دو معماری خیلی عمیق هستند ، در طول عملیات انتشار رو به عقب(محاسبه مشتق) مسیر کوتاه تری برای انجام عملیات بایستی طی شود.کد این مقاله در [این لینک](https://github.com/gustavla/fractalnet) قابل دانلود است اما کدی که توسط نویسندگان مقاله ارایه شده فقط شامل drop-path محلی است و کد مربوط به drop—drop سراسری منتشر نشده است. این مدل در فریم ورک Keras نیز پیاده سازی شده است . به عنوان مثال شمای یک مدل برای آموزش بر روی دیتابیس CIFAR-10 به این شکل است.[کلیک کنید ](https://raw.githubusercontent.com/snf/keras- fractalnet/master/doc/model.png).

**مقاله سوم : [Residual Networks of Residual Networks](https://arxiv.org/abs/1608.02908)**

این مقاله در  واقع ترکیبی از چند مقاله توضیح داده شده قبلی است و از متد هایی مثل drop-path، stochastic depth در آن برای جلوگیری از بیش بر ارزش شدن استفاده شده است. تفاوت عمده این معماری با معماری ResNet استفاده از skip connection برای چندین بلاک Residual است . این معماری نتایج قبلی را به شکل زیر بهبود داد `CIFAR-10 (3.77%), CIFAR-100 (19.73%) and SVHN (1.59%)`
[کد](https://github.com/titu1994/Residual-of-Residual-Networks)

![_config.yml]({{ site.baseurl }}/images/13/4.png)

**مقاله چهارم : Densely Connected Convolutional Networks(https://arxiv.org/abs/1608.06993)**

DenseNet یکی از آخرین شبکه های عصبی هست که برای بازتشخصی اشیاء ارائه شد. این معماری نیز کاملا مشابه معماری ResNet ،اما دارای چند تفاوت اساسی است. این معماری نسبت به سایر معماری های قبلی نرخ خطای کمتری بر روی دیتا بیس های CIFAR و SVHN دارد . در جدول زیر نتایج مقاله هایی که تا کنون بررسی شده قرار گرقته است.

![_config.yml]({{ site.baseurl }}/images/13/5.png)

همچنین نتایج بدست آمده نشان میدهد که این معماری برای دیتابیس ImageNet به تعداد پارامتر کمتری نیاز نسبت به معماری ResNet نیاز دارد در حالی که دقت آنها مشابه است.

![_config.yml]({{ site.baseurl }}/images/13/6.png)

فرق عمده این متد با متد های قبلی استفاده از skip connection برای تمامی لایه ها و لایه های ماقبل آن است ، با این ایده که اگر با اتصال لایه قبل به لایه های بعد آن توسط  skip connection باعث افزایش کارایی میشود چرا همه آنها را به هم وصل نکنیم ؟ با این ایده اطلاعات می‌تواند در انتشار رو به عقب به صورت مستقیم در مسیر شبکه حرکت کنند ‌.

![_config.yml]({{ site.baseurl }}/images/13/7.png)

اما چه توجیهی برای این ساختار وجود دارد ؟ همانطور که میدانید در شبکه های عصبی کانولوشن لایه های ابتدایی ویژگی های سطح پایین مثل لبه ها را استخراج می‌کنند و لایه هایی که در انتهای این زنجیره قرار گرفتند ویژگی‌های سطح بالا مثل بافت ها ، مثل صورت و غیره را استخراج می‌کنند . با توجه به توضیحات ذکر شده در برخی موارد ممکن است در عملیات طبقه بندی یک کلاس از ویژگی های سطح پایین سود بیشتری ببرد ، با توجه به متصل بودن لایه هایی ابتدایی به لایه های انتهایی شبکه می‌تواند یاد بگیرد که برای کلاس مورد نظر فقط از ویژگی های سطح پایین استفاده کند ، همچنین هر لایه دسترسی مستقیم به مشتق تابع هزینه و ورودی اصلی شبکه دارد این مورد کمک به آموزش معماری های عمیق تر میکند.

فرض کنید یک عکس `x0` از میان یک شبکه عصبی کانولوشن عبور میکند. شبکه شامل چند لایه(L) است و هر کدام از این لایه ها یک یکی سری تبدیلات غیر خطی `Hl(.)` انجام میدهند(در اینجا ایندکس `l`  به لایه مورد نظر اشاره دارد. `Hl(.)` میتواند ترکیبی از عملیات Batch Normalization، ReLU ، Pooling یا کانولوشن باشد.خروجی لایه l ام توسط xl نمایش داده شده است.

در یک شبکه کانولوشنال معمولی شبکه خروجی لایه l ام را به عنوان ورودی به لایه بعدی یعنی `(l+1)^th`  میدهد که با عبور از تابع غیرخطی به شکل `xl= Hl(xl−1)` در میاید. رد یک شبکه ResNet این مقدار با یک تابع همانی جمع میشود (که همان skip connection است). در هنگام الگوریتم انتشار رو به عقب گرادیان به صورت مستقیم از میان تابع همانی از یک لایه به لایه قبلی عبور میکند .

![_config.yml]({{ site.baseurl }}/images/13/8.png)

در معماری DenseNet برا بهبود انتقال اطلاعات بین لایه ها نویسندگان یک الگوی اتصال متفاوت را ارایه میدهند که اتصال هر لایه به تمامی لایه های بعد آن است.در نتیجه لایه l ام نقشه ویژگی(feature map) تمامی لایه های پردازش شده را به عنوان ورودی دریافت میکند:

![_config.yml]({{ site.baseurl }}/images/13/9.png)

به طوری که مجموعه x ها به الحاق نقشه ویژگی لایه های قبلی اشاره دارند. در بین هر کدام از بلاک های Dense برای کنترل ابعاد (طول و عرض) از یک pooling   2 ×2 به همراه یک کانولوشن با اندازه 1 ×1  استفاده میشود.

![_config.yml]({{ site.baseurl }}/images/13/10.png)

در انتها یکی از تفاوت های دیگر ResNet و DenseNet این است که ورودی  Hl(•) به جای جمع شدن با یکدیگر ترکیب میشوند.

![_config.yml]({{ site.baseurl }}/images/13/11.png)

کد این مقاله در [این لینک](https://github.com/liuzhuang13/DenseNet) قرار گرفته است.
