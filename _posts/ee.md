---
layout: post
title: معماری MobileNetsو Xception (بخش اول)
---
مقاله ای که اخیرا توسط گوگل منتشر شد یک معماری جدید است به نام MobileNets که برای [سیستم هایی تعبیه شده](https://en.wikipedia.org/wiki/Embedded_system) مثل موبایل بهینه سازی شده است. در هسته اصلی این معماری کانولوشن بر حسب کانال تفکیک پذیر (depthwise separable convolution) برای ساخت یک شبکه عصبی سبک استفاده شده است .

من همیشه به شبکه های عصبی کانولوشنال سبک (lightweight) علاقه مند بودم، چند روز پیش تصمیم گرفتم که مقاله [MobileNets](https://arxiv.org/abs/1704.04861) را مطالعه کنم اما در میانه راه مجبور شدم مقاله [Xception](https://arxiv.org/abs/1610.02357) ، که توسط François Chollet (نویسنده [کتابخانه متن باز شبکه عصبی کراس](https://en.wikipedia.org/wiki/Keras) و موسس [Wysp](https://www.wysp.ws/) که یک پلت‌فرم برای هنرمندان است) منتشر شده است، را بخوانم . این پست در واقع جمع بندی این دو مقاله است. با شکل گیری شاخه یادگیری عمیق اصطلاحات زیادی به وجود آمد که معادل فارسی برای آنها وجود ندارند (یا من از آنها بی اطلاع هستم 😟 ). در این پست سعی کردم برای هر کدام از این عبارات معادل فارسی که توصیف کننده آن کلمه هست استفاده کنم ، در صورتی که معال بهتری برای آنها در ذهن دارید یا قبلا وجود داشته است لطفا به من اطلاع دهید تا اصلاح کنم. 🙏🏽

در هر دوی این مقاله برای کاهش تعداد پارامتر های شبکه کانولوشنال از دو عملیات کانولوشن به نام های کانولوشن نقطه ای(pointwise) و کانولوشن بر حسب کانال(depthwise) استفاده میشود که در حین کاهش قابل توجه پارامتر ها دقت نیز در حد مطلوب باقی میماند. به عبارتی بده و بستان بین دقت و پارامتر ها بهبود پیدا میکند!

![_config.yml]({{ site.baseurl }}/images/9/1.png)

ساخت یک شبکه سبک وزن(یک شبکه با تعداد پارامتر کمتر) برای تلفن های همراه، دستگاه هایی که قدرت محاسباتی آنها محدود است یک زمینه باز در مبحث یادگیری عمیق است. با کاهش تعداد پارامتر ها این امکان نیز وجود دارد که شبکه ای عمیق تر ایجاد کنیم در نتیجه دقت نیز میتواند افزایش پیدا کند .  

متد های زیادی برای ایجاد این چنین شبکه هایی وجود دارند، اما میتوان آنها را به دو بخش عمده تقسیم کرد: 
1-	کاهش تعداد پارامتر ها با ابداع ساختار جدید لایه کانولوشن 
به عنوان مثال متد هایی که در مقاله های [GoogLeNet](https://arxiv.org/abs/1409.4842)،[ResNet](https://arxiv.org/abs/1512.03385)، [SqueezeNet](https://arxiv.org/abs/1602.07360)،[MobileNets](https://arxiv.org/abs/1704.04861) ،[Xception](https://arxiv.org/abs/1704.04861) و[ShuffleNet](https://arxiv.org/pdf/1707.01083.pdf)معرفی شدند.
2-	فشرده سازی پارامتر ها
به عنوان مثال میتوان به مقاله های مثل [BinaryConnect](https://arxiv.org/abs/1511.00363)،[Binarized Neural Networks](https://arxiv.org/abs/1602.02830)،[EIE](https://arxiv.org/abs/1602.01528)،[Deep Compression](https://arxiv.org/abs/1510.00149) و [Compressing Deep Convolutional Networks using Vector Quantization](https://arxiv.org/abs/1412.6115) اشاره کرد.

در واقع مقالات زیادی در چند اخیر در این زمینه منتشر شده است که تعدادی از آنها را به عنوان مرجع در بالا آوردم.

هنگامی که مقاله GoogLeNet در سال 2014 منتشر شد شامل ماژول هایی Inception مثل شکل زیر بود:

 ![_config.yml]({{ site.baseurl }}/images/9/2.png)

ایده اصلی پشت این ماژول این است که ابتدا توسط یک کانولوشن با اندازه فیلتر 1x1 همبستگی میان کانالی(cross-channel)و سپس همبستگی مکانی(spatial correlations) توسط یک کانولوشن یا اندازه فیلتر 3x3 محاسبه میشود. این دو عملیات جدا از یکدیگر انجام میشوند ، ایده اصلی استفاده از کانولوشن با اندازه فیلتر 1x1 به این دلیل است که این فیلتر یک پیکسل و تمامی کانال های آن (به عنوان مثال در یک عکس این کانال ها به سه کانال `R و G و B` اشاره دارند) را به یک پیکسل خروجی نگاشت میدهد. از این عملیات عمدتا برای کاهش تعداد ابعاد فیلتر(عمق) استفاده استفاده میشود .

کانولوشن بر حسب کانال (Depthwise) و نقطه ای(pointwise)

برای درک شایستگی و ارزشمند بودن کانولوشن بر حسب کانال و نقطه ای ابتدا نیاز است که پیچیدگی محاسباتی لایه کانولوشن و تعداد پارامتر ها را بررسی کرد.نماد هایی که در ادامه استفاده خواهد شد متفاوت از متن خود مقالات است!

برای درک بهتر این عملیات مثال را در نظر بگیرید :در یک لایه کانولوشن، فرض کنید مقادیر زیر را دارید:

```javascript
اندازه نقشه ویژگی ورودی:W x H
تعداد کانال های ورودی : N
اندازه فیلتر:K xK
تعداد کانال های خروجی: M(تعداد فیلتر ها)
```

با توجه به موارد ذکر شده در مثال بالا، پیچیدگی محاسباتی یک لایه کانولوشن معمولی
  `W xH xN xK^2 xM` خواهد بود. به عبارتی هزینه عملیات کانولوشن برای هر مکان در  نقشه ویژگی ورودی `K^2xN`،اعمال کردن این عملیات به  نقشه ویژگی ورودی با `WxH` مکان باعث ایجاد یک نقشه ویژگی از یک کانال میشود از طرفی خروجی ما دارای `M` کانال است .

تعداد پارامتر ها: `M` نوع کانولوشن با تعداد پارامتر `K^2xN`  وجود دارد که نتیجه آن `K^2xNxM` خواهد شد.

![_config.yml]({{ site.baseurl }}/images/9/3.png)

کانولوشن نقطه ای در واقع همان کانولوشن 1x1 است که اولین بار در GoogLeNet استفاده شد. *نکته* این عملیات در مسیر عمق کانال ورودی انجام میشود . از این عملیات برای کاهش ابعاد نقشه ویژگی استفاده میشود. از آنجایی که کانولوشن نقطه ای در واقع مشابه یک کانولوشن معمولی با اندازه فیلتر `K=1` است پیچیدگی محاسبات `W xH xN xM` و تعداد پارامتر ها `NxM` خواهد بود.
![_config.yml]({{ site.baseurl }}/images/9/4.png)

کانولوشن بر حسب کانال عملیات را در مسیر فضایی به طور مجزا برای هر کدام کانال نقشه ویژگی  به صورت مجزا انجام میدهد.پیچیدگی محاسباتی `WxHxNxK^2` و تعداد پارامتر ها `K^2xN` خواهد بود.

![_config.yml]({{ site.baseurl }}/images/9/5.png)

با ترکیب دو عملیات ذکر شده در بالا میتوان عملیات کانولوشن در یک لایه را با تعداد پارامتر و هزینه محاسباتی به مراتب کمتری انجام داد.با توجه به توضحات ذکر شده پیچیدگی محاسباتی از `WxHxNxK^2xM` به `WxHxNxM+WxHxNxK^2` کاهش پیدا میکند. فرض کنید یک عکس 48 در 48 پیکسل با سه کانال RGB و 64 فیلتر 3x3 داریم پیچیدگی محاسباتی یک کانولوشن معمولی 3,981,312 و پیچیدگی محاسباتی با روش جدید 504,576 خواهد بود این یعنی نزدیک به 8 برابر کمتر!

دو مقاله Xception و MobileNets از این عملیات به جای کانولوشن معمولی استفاده میکنند.بیایید نگاهی به این دو معماری بیندازیم.

معماری Xception

![_config.yml]({{ site.baseurl }}/images/9/6.png)

معماری MobileNet

![_config.yml]({{ site.baseurl }}/images/9/7.png)

در دو شکل بالا این دو معماری نمایش داده شده است . در شکل اول که مربوط به معماری Xception است دیتا از سه قسمت عبور میکنند ، که بخش میانی 8 بار تکرار شده است . بعد از هر لایه depthwise یک لایه pointwise و batch normalization قرار گرفته است که در شکل نمایش داده نشده است.مقایسه این دو مدل در نگاه اول غیر ممکن به نظر میرسد به همین علت ما آنها را بر اساس بلاک مقایسه میکنیم .

ماژل MobileNets :از قسمت های ذیر تشکیل شده است :

```python
depthwise->batch normalization->ReLU->pointwise->batch normalization->ReLU
```

![_config.yml]({{ site.baseurl }}/images/9/8.png)


ماژول Xception  از قسمت های ذیل تشکیل شده است:

```python
ReLU->depthwise->pointwise->batch normalization->ReLU->depthwise->pointwise->batch normalization->ReLU->depthwise->pointwise->batch normalization+identity mapping
```

اما تفاوت عمده:

ماژول Xception  دارای مفهمومی بنام skip connection است که اولین بار در معماری ResNet معرفی شد. 

در ماژول Xception  هیچ تابع فعال ساز بین کانولوشن depthwise  و pointwise  وجود ندارد.

کد مقاله MobileNet در [این لینک ](https://github.com/lyk125/MobileNet-1) قرار گرفته است، همچنین در تنسورفلو برای استفاده از عملیات کانولوشن پیشنهادی میتوانید از تابع separable_conv2dاستفاده کنیدکه ابتدا عملیات کانولوشن بر حسب کانال به صورت جداگانه بر روی هر کانال ورودی و سپس عملیات کانولوشن نقطه ای را بر روی انجام میدهد:

```python
tf.nn.separable_conv2d(input, depthwise_filter, pointwise_filter, strides, padding, rate=None, name=None)
```

در پایان نکات جالبی است که در مقاله قید نشده است را برایتان بازگو میکنم :

این متد اولین بار توسط دکتر Laurent Sifre در سال 2013 کشف شد و در معماری الکس نت اعمال شد اما در این مقاله چیزی در این باره قید نشده است! به عنوان مرجع [فصل ششم،بخش دوم پایان نامه دکترا ایشان](http://www.di.ens.fr/data/publications/papers/phd_sifre.pdf) را مطالعه کنید. این متد همچنین در کنفرانس [ICLR 2014](https://sites.google.com/site/representationlearning2014/) توسط [Vincent Vanhoucke](https://research.google.com/pubs/VincentVanhoucke.html) شرح داده شد . برای جزییات بیشتر دقیقه 20 [این ویدیو](https://youtu.be/VhLe-u0M1a8?t=1213) را ببینید

![_config.yml]({{ site.baseurl }}/images/9/9.png)
